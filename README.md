# 🔍 Project: Toxic Comment Detector

A simple ML web app to classify comments as **Toxic** or **Clean**, using text input or `.txt` files.

---

## 🚀 Features

- ✅ Detects toxic/hateful comments  
- 📁 Accepts single-line input or bulk `.txt` files  
- 📊 Shows visual summary (percentages, pie chart)  
- 💻 Built with: `Scikit-learn`, `TF-IDF`, `Streamlit`  

---

## 🧠 Model Info

- **Model**: Logistic Regression (optional: Random Forest / XGBoost)  
- **Text Vectorization**: TF-IDF  
- **Training Data**: [Kaggle Jigsaw Toxic Comment Dataset](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)  

---



## ▶️ How to Use

### 1. Clone the Repository
```bash
git clone https://github.com/your-username/Toxic-Comment-Detector.git
cd Toxic-Comment-Detector
### Install Dependencies
pip install -r requirements.txt
###Run the Streamlit App
streamlit run app.py
###Acknowledgements
Kaggle Jigsaw Toxic Comment Challenge

Built using Python, Scikit-learn, Streamlit

Let me know if you want me to push this to a `README.md` file in your project structure or if you want it styled with emojis or GitHub-flavored markdown!
Link dataser : https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data?select=test.csv.zip
Link website : https://toxic-comment-detector09.streamlit.app/



