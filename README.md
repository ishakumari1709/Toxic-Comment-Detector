# ğŸ” Project: Toxic Comment Detector

A simple ML web app to classify comments as **Toxic** or **Clean**, using text input or `.txt` files.

---

## ğŸš€ Features

- âœ… Detects toxic/hateful comments  
- ğŸ“ Accepts single-line input or bulk `.txt` files  
- ğŸ“Š Shows visual summary (percentages, pie chart)  
- ğŸ’» Built with: `Scikit-learn`, `TF-IDF`, `Streamlit`  

---

## ğŸ§  Model Info

- **Model**: Logistic Regression (optional: Random Forest / XGBoost)  
- **Text Vectorization**: TF-IDF  
- **Training Data**: [Kaggle Jigsaw Toxic Comment Dataset](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)  

---



## â–¶ï¸ How to Use

### 1. Clone the Repository
```bash
git clone https://github.com/your-username/Toxic-Comment-Detector.git
cd Toxic-Comment-Detector
### Install Dependencies
pip install -r requirements.txt
###Run the Streamlit App
streamlit run app.py
###Acknowledgements
Kaggle Jigsaw Toxic Comment Challenge

Built using Python, Scikit-learn, Streamlit

Let me know if you want me to push this to a `README.md` file in your project structure or if you want it styled with emojis or GitHub-flavored markdown!
Link dataser : https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data?select=test.csv.zip
Link website : https://toxic-comment-detector09.streamlit.app/



